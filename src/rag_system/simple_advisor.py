"""
ë²¡í„°DB ì—†ì´ ì‘ë™í•˜ëŠ” ê°„ì†Œí™”ëœ ì–´ë“œë°”ì´ì € (í”„ë¡¬í”„íŠ¸ ì˜¤ë¥˜ ìˆ˜ì •)
"""
import os
from typing import Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage
from loguru import logger

class SimpleTrafficAdvisor:
    def __init__(self, google_api_key: str, model_name: str = "gemini-1.5-flash"):
        """
        ê°„ì†Œí™”ëœ êµí†µì‚¬ê³  ìë¬¸ì‚¬ (ë²¡í„°DB ë¶ˆí•„ìš”)
        """
        self.google_api_key = google_api_key
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=google_api_key,
            temperature=0.2,
            convert_system_message_to_human=True
        )
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ConversationBufferWindowMemory(
            k=3,
            memory_key="chat_history",
            return_messages=True
        )
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self._setup_prompts()
        
        logger.info("ê°„ì†Œí™”ëœ êµí†µì‚¬ê³  ìë¬¸ì‚¬ ì´ˆê¸°í™” ì™„ë£Œ (ë²¡í„°DB ì—†ìŒ)")
    
    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ ì„¤ì • (ì¤‘ê´„í˜¸ ë¬¸ì œ í•´ê²°)"""
        
        # ë¹ ë¥¸ ì§„ë‹¨ í”„ë¡¬í”„íŠ¸ (ì¤‘ê´„í˜¸ ì œê±°)
        self.quick_diagnosis_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ë²•ë¥  ìë¬¸ì‚¬ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìƒí™©: {user_input}
ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: {image_analysis}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ğŸš— ì‚¬ê³  ìƒí™© ìš”ì•½
ìƒí™©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

## âš–ï¸ ì˜ˆìƒ ê³¼ì‹¤ë¹„ìœ¨
ì¼ë°˜ì ì¸ ê³¼ì‹¤ë¹„ìœ¨ê³¼ ê°„ë‹¨í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

## ğŸ“‹ ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­
1. ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¼
2. ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ì¡°ì¹˜
3. ì„¸ ë²ˆì§¸ ì¡°ì¹˜ì‚¬í•­

## ğŸ’¡ ì£¼ìš” í¬ì¸íŠ¸
ê³¼ì‹¤ ë¹„ìœ¨ì— ì˜í–¥ì„ ì£¼ëŠ” í•µì‹¬ ìš”ì†Œ 1-2ê°œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

---
**ë” ìì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ "ìƒì„¸ ë¶„ì„"ì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”.**

âš ï¸ ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
""")
        
        # ìƒì„¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.detailed_analysis_prompt = ChatPromptTemplate.from_template("""
ì‚¬ìš©ìê°€ ìƒì„¸ ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ ëŒ€í™”: {chat_history}
ì‚¬ìš©ì ìš”ì²­: {user_input}

ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ” ë²•ë¥ ì  ë¶„ì„
- ì ìš© ê°€ëŠ¥í•œ ë„ë¡œêµí†µë²• ì¡°í•­
- ê³¼ì‹¤ ë¹„ìœ¨ ê²°ì • ìš”ì¸

## ğŸ“Š ê³¼ì‹¤ë¹„ìœ¨ ìƒì„¸ ë¶„ì„
- Aì°¨ëŸ‰ ê³¼ì‹¤ ìš”ì¸
- Bì°¨ëŸ‰ ê³¼ì‹¤ ìš”ì¸
- ì¡°ì • ê°€ëŠ¥í•œ ìš”ì†Œë“¤

## ğŸ›¡ï¸ ëŒ€ì‘ ì „ëµ
- ë³´í—˜ì‚¬ ëŒ€ì‘ ë°©ë²•
- í•„ìš”í•œ ì¦ê±° ìë£Œ
- ì£¼ì˜ì‚¬í•­

## â“ ì¶”ê°€ ê³ ë ¤ì‚¬í•­
ì •í™•í•œ íŒë‹¨ì„ ìœ„í•´ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì•Œë ¤ì£¼ì„¸ìš”.

í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.
""")
    
    def quick_diagnosis(self, user_input: str, image_analysis: str = "") -> Dict[str, Any]:
        """ë¹ ë¥¸ ì§„ë‹¨"""
        try:
            chain = LLMChain(llm=self.llm, prompt=self.quick_diagnosis_prompt)
            
            result = chain.invoke({
                "user_input": user_input,
                "image_analysis": image_analysis or "ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
            })
            
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.memory.chat_memory.add_user_message(user_input)
            self.memory.chat_memory.add_ai_message(result["text"])
            
            return {
                "answer": result["text"],
                "type": "quick_diagnosis",
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ë¹ ë¥¸ ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "type": "error",
                "status": "error",
                "error": str(e)
            }
    
    def detailed_analysis(self, user_input: str) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„"""
        try:
            # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            chat_history = "\n".join([
                f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ìë¬¸ì‚¬'}: {msg.content}"
                for msg in self.memory.chat_memory.messages[-6:]
            ])
            
            chain = LLMChain(llm=self.llm, prompt=self.detailed_analysis_prompt)
            
            result = chain.invoke({
                "chat_history": chat_history,
                "user_input": user_input
            })
            
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.memory.chat_memory.add_user_message(user_input)
            self.memory.chat_memory.add_ai_message(result["text"])
            
            return {
                "answer": result["text"],
                "type": "detailed_analysis",
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "answer": "ìƒì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "type": "error",
                "status": "error",
                "error": str(e)
            }
    
    def generate_follow_up_questions(self, user_input: str, chat_history: str = "") -> Dict[str, Any]:
        """ë¶€ì¡±í•œ ì •ë³´ë¥¼ ìœ„í•œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±"""
        try:
            follow_up_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ì¡°ì‚¬ì›ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ íŒŒì•…í•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”.

í˜„ì¬ê¹Œì§€ì˜ ì •ë³´:
{user_input}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ë‹¤ìŒ ì˜ì—­ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 2-3ê°œì˜ ì§ˆë¬¸ì„ ì„ íƒí•´ì„œ ë¬¼ì–´ë³´ì„¸ìš”:

**í•„ìˆ˜ ì •ë³´ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- ì‚¬ê³  ë°œìƒ ì‹œê°„ê³¼ ë‚ ì”¨ (ì£¼ê°„/ì•¼ê°„, ë§‘ìŒ/ë¹„/ëˆˆ)
- ì •í™•í•œ ì‚¬ê³  ìœ„ì¹˜ (êµì°¨ë¡œ ì¢…ë¥˜, ì°¨ì„  ìˆ˜, ì‹ í˜¸ë“± ìƒí™©)
- ì–‘ ì°¨ëŸ‰ì˜ ì£¼í–‰ ìƒí™© (ì†ë„, ë°©í–¥, ìš°ì„ ê¶Œ)
- ì¶©ëŒ ë²”ìœ„ì™€ ì†ìƒ ì •ë„
- ëª©ê²©ìë‚˜ ë¸”ë™ë°•ìŠ¤ ì¡´ì¬ ì—¬ë¶€
- ì‚¬ê³  ë‹¹ì‹œ ìŒì£¼ ì—¬ë¶€
- ì°¨ëŸ‰ ì •ë¹„ ìƒíƒœ (íƒ€ì´ì–´, ë¸Œë ˆì´í¬ ë“±)

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ğŸ” ì¶”ê°€ ì •ë³´ í•„ìš”
ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ëª½ ê°€ì§€ ë” ì•Œë ¤ì£¼ì„¸ìš”:

**1. [ì²« ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì‚¬ê³  ë‹¹ì‹œ ì •í™•í•œ ì‹œê°„ê³¼ ë‚ ì”¨ëŠ” ì–´ë– í–ˆë‚˜ìš”?

**2. [ë‘ ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì‚¬ê³  ì§€ì ì˜ ì‹ í˜¸ë“± ìƒí™©ì€ ì–´ë– í–ˆë‚˜ìš”?

**3. [ì„¸ ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì–‘ ì°¨ëŸ‰ì˜ ì£¼í–‰ ì†ë„ëŠ” ëŒ€ëµ ì–¼ë§ˆì˜€ë‚˜ìš”?

ì´ ì •ë³´ë“¤ì„ ì•Œë ¤ì£¼ì‹œë©´ í›¨ì”¬ ë” ì •í™•í•œ ê³¼ì‹¤ë¹„ìœ¨ ë¶„ì„ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
            
            chain = LLMChain(llm=self.llm, prompt=follow_up_prompt)
            
            result = chain.invoke({
                "user_input": user_input,
                "chat_history": chat_history or "ëŒ€í™” ê¸°ë¡ ì—†ìŒ"
            })
            
            return {
                "questions": result["text"],
                "type": "follow_up_questions",
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "questions": "ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "type": "error",
                "status": "error"
            }
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„"""
        
        # ìƒì„¸ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        detail_keywords = ["ìƒì„¸", "ë” ìì„¸íˆ", "ìƒì„¸ ë¶„ì„", "ë” ì•Œê³  ì‹¶", "ìì„¸í•œ ì„¤ëª…"]
        is_detail_request = any(keyword in user_input for keyword in detail_keywords)
        
        if is_detail_request:
            return self.detailed_analysis(user_input)
        else:
            return self.quick_diagnosis(user_input, image_analysis)
    
    def analyze_with_context(self, user_input: str, image_analysis: str = "") -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„"""
        
        # ìƒì„¸ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        detail_keywords = ["ìƒì„¸", "ë” ìì„¸íˆ", "ìƒì„¸ ë¶„ì„", "ë” ì•Œê³  ì‹¶", "ìì„¸í•œ ì„¤ëª…"]
        is_detail_request = any(keyword in user_input for keyword in detail_keywords)
        
        if is_detail_request:
            return self.detailed_analysis(user_input)
        else:
            return self.quick_diagnosis(user_input, image_analysis)
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear()
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

def test_simple_advisor():
    """ê°„ì†Œí™”ëœ ìë¬¸ì‚¬ í…ŒìŠ¤íŠ¸"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    if not google_api_key:
        print("GOOGLE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        return
    
    advisor = SimpleTrafficAdvisor(google_api_key)
    
    print("ğŸš— ê°„ì†Œí™”ëœ êµí†µì‚¬ê³  ìë¬¸ì‚¬ í…ŒìŠ¤íŠ¸")
    test_case = "ì‹ í˜¸ë“± êµì°¨ë¡œì—ì„œ ì§ì§„ ì¤‘ ì¢ŒíšŒì „ ì°¨ëŸ‰ê³¼ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤"
    
    result = advisor.quick_diagnosis(test_case)
    print("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if result["status"] == "success" else "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    if result["status"] == "success":
        print("ë‹µë³€ ì¼ë¶€:")
        print(result["answer"][:200] + "...")

if __name__ == "__main__":
    test_simple_advisor()
