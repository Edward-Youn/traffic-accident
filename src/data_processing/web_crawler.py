import requests
from bs4 import BeautifulSoup
import json
import time
import urllib3
import os
from loguru import logger
from pathlib import Path

class AccidentCrawler:
    def __init__(self, output_dir="./data/cases"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.base_url = "https://accident.knia.or.kr/myaccident-content"
        
        # SSL ì¸ì¦ì„œ ê²½ê³  ë¬´ì‹œ
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ë¡œê¹… ì„¤ì •
        logger.add("./logs/crawler.log", rotation="10 MB")
    
    def crawl_accident_data(self, max_pages=62, delay=1):
        """êµí†µì‚¬ê³  íŒë¡€ ë°ì´í„° í¬ë¡¤ë§"""
        accident_data = []
        total_cases = 0
        
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        for page_number in range(1, max_pages + 1):
            sub_page_number = 1
            page_cases = 0
            
            while True:
                try:
                    # URL ìƒì„±
                    params = {
                        'chartNo': f'ì°¨{page_number}-{sub_page_number}',
                        'chartType': '1'
                    }
                    
                    # í˜ì´ì§€ ìš”ì²­
                    response = requests.get(
                        self.base_url, 
                        params=params,
                        verify=False,
                        timeout=10
                    )
                    
                    logger.debug(f"ìš”ì²­ URL: {response.url}")
                    
                    # í˜ì´ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
                    if "ìš”ì²­í•˜ì‹  í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in response.text:
                        logger.debug(f"í˜ì´ì§€ {page_number} ì™„ë£Œ (ì´ {page_cases}ê±´)")
                        break
                    
                    # ë°ì´í„° ì¶”ì¶œ
                    case_data = self._extract_case_data(response.text, page_number, sub_page_number)
                    
                    if case_data:
                        accident_data.append(case_data)
                        page_cases += 1
                        total_cases += 1
                        logger.info(f"ì¶”ì¶œ ì™„ë£Œ: ì°¨{page_number}-{sub_page_number}")
                    
                    sub_page_number += 1
                    time.sleep(delay)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except requests.RequestException as e:
                    logger.error(f"ìš”ì²­ ì‹¤íŒ¨ ì°¨{page_number}-{sub_page_number}: {e}")
                    break
                except Exception as e:
                    logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜ ì°¨{page_number}-{sub_page_number}: {e}")
                    sub_page_number += 1
                    continue
            
            logger.info(f"í˜ì´ì§€ {page_number}/{max_pages} ì™„ë£Œ (í˜„ì¬ê¹Œì§€ ì´ {total_cases}ê±´)")
        
        # ë°ì´í„° ì €ì¥
        output_file = self.output_dir / "accident_cases.json"
        self._save_data(accident_data, output_file)
        
        logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_cases}ê±´ ìˆ˜ì§‘, {output_file}ì— ì €ì¥")
        return accident_data
    
    def _extract_case_data(self, html_content, page_num, sub_page_num):
        """HTMLì—ì„œ ì‚¬ê³  ë°ì´í„° ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, "html.parser")
            
            # ê° ìš”ì†Œ ì¶”ì¶œ
            car_A = soup.select_one(".cont_l .con")
            car_B = soup.select_one(".cont_r .con")
            accident_description = soup.select_one("#smrizeexplna")
            fault_A = soup.select_one("td .red")
            fault_B = soup.select_one("td .orange")
            
            # ëª¨ë“  ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
            if not all([car_A, car_B, accident_description, fault_A, fault_B]):
                logger.warning(f"ì°¨{page_num}-{sub_page_num}: ì¼ë¶€ ë°ì´í„° ëˆ„ë½")
                return None
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            description_text = self._clean_text(accident_description.text)
            
            return {
                "case_id": f"ì°¨{page_num}-{sub_page_num}",
                "vehicle_A_situation": car_A.text.strip(),
                "vehicle_B_situation": car_B.text.strip(),
                "accident_description": description_text,
                "fault_ratio": f"{fault_A.text.strip()} : {fault_B.text.strip()}",
                "page_number": page_num,
                "sub_page_number": sub_page_num,
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ ì°¨{page_num}-{sub_page_num}: {e}")
            return None
    
    def _clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±° ë° íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        cleaned = text.strip()
        cleaned = cleaned.replace("\r\n", " ").replace("\n", " ")
        cleaned = cleaned.replace("âŠ™ ", "")
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        import re
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        return cleaned.strip()
    
    def _save_data(self, data, output_file):
        """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = AccidentCrawler()
    
    # í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5í˜ì´ì§€ë§Œ)
    print("ğŸ•·ï¸ êµí†µì‚¬ê³  íŒë¡€ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("âš ï¸ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ì—ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() != 'y':
        print("í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # í˜ì´ì§€ ìˆ˜ ì„¤ì •
    try:
        max_pages = int(input("í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 5, ì „ì²´: 62): ") or "5")
    except ValueError:
        max_pages = 5
    
    accident_data = crawler.crawl_accident_data(max_pages=max_pages)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(accident_data)}ê±´ì˜ ì‚¬ê³  íŒë¡€ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()