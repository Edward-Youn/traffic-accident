"""
íŒë¡€ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê°œì„ ëœ ìë¬¸ì‚¬
ë²¡í„°DB ì—†ì´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
"""
import os
import json
from typing import Dict, Any, List
from pathlib import Path
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage
from loguru import logger

class EnhancedTrafficAdvisor:
    def __init__(self, google_api_key: str, model_name: str = "gemini-1.5-flash"):
        """
        íŒë¡€ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” êµí†µì‚¬ê³  ìë¬¸ì‚¬
        """
        self.google_api_key = google_api_key
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=google_api_key,
            temperature=0.2,
            convert_system_message_to_human=True
        )
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ConversationBufferWindowMemory(
            k=3,
            memory_key="chat_history",
            return_messages=True
        )
        
        # íŒë¡€ ë°ì´í„° ë¡œë“œ
        self.case_data = self._load_case_data()
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self._setup_prompts()
        
        logger.info(f"íŒë¡€ ë°ì´í„° í™œìš© ìë¬¸ì‚¬ ì´ˆê¸°í™” ì™„ë£Œ ({len(self.case_data)}ê±´ íŒë¡€)")
    
    def _load_case_data(self) -> List[Dict]:
        """íŒë¡€ ë°ì´í„° ë¡œë“œ"""
        try:
            case_file = Path("./data/cases/accident_cases.json")
            if case_file.exists():
                with open(case_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning("íŒë¡€ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
        except Exception as e:
            logger.error(f"íŒë¡€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _search_similar_cases(self, user_input: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰"""
        if not self.case_data:
            return []
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = ["ì§ì§„", "ì¢ŒíšŒì „", "ìš°íšŒì „", "ì‹ í˜¸ë“±", "êµì°¨ë¡œ", "ë…¹ìƒ‰", "ì ìƒ‰", "í™©ìƒ‰", 
                   "ë¹„ë³´í˜¸", "í™”ì‚´í‘œ", "ì ë©¸", "í›„ì§„", "ì°¨ì„ ë³€ê²½", "ì£¼ì°¨ì¥"]
        
        user_keywords = [kw for kw in keywords if kw in user_input]
        
        if not user_keywords:
            return []
        
        # ìœ ì‚¬ ì‚¬ë¡€ ì°¾ê¸°
        similar_cases = []
        for case in self.case_data:
            score = 0
            case_text = f"{case['vehicle_A_situation']} {case['vehicle_B_situation']} {case['accident_description']}"
            
            for keyword in user_keywords:
                if keyword in case_text:
                    score += 1
            
            if score > 0:
                case['similarity_score'] = score
                similar_cases.append(case)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 3ê°œ ë°˜í™˜
        similar_cases.sort(key=lambda x: x['similarity_score'], reverse=True)
        return similar_cases[:3]
    
    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        
        # íŒë¡€ í™œìš© ì§„ë‹¨ í”„ë¡¬í”„íŠ¸
        self.enhanced_diagnosis_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ë²•ë¥  ìë¬¸ì‚¬ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìƒí™©: {user_input}
ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: {image_analysis}

**ê´€ë ¨ íŒë¡€ ì •ë³´:**
{similar_cases}

ìœ„ íŒë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ğŸš— ì‚¬ê³  ìƒí™© ìš”ì•½
ìƒí™©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

## âš–ï¸ ì˜ˆìƒ ê³¼ì‹¤ë¹„ìœ¨
**ìœ ì‚¬ íŒë¡€ ê¸°ì¤€:** {case_examples}
**ì˜ˆìƒ ê³¼ì‹¤ë¹„ìœ¨:** [íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜ˆìƒ ë¹„ìœ¨ê³¼ ê·¼ê±°]

## ğŸ“‹ ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­
1. ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¼
2. ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ì¡°ì¹˜
3. ì„¸ ë²ˆì§¸ ì¡°ì¹˜ì‚¬í•­

## ğŸ’¡ ì£¼ìš” í¬ì¸íŠ¸
ê³¼ì‹¤ ë¹„ìœ¨ì— ì˜í–¥ì„ ì£¼ëŠ” í•µì‹¬ ìš”ì†Œë¥¼ íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

## ğŸ“š ì°¸ê³  íŒë¡€
{reference_cases}

---
**ë” ìì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ "ìƒì„¸ ë¶„ì„"ì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”.**

âš ï¸ ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
""")
        
        # ìƒì„¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.detailed_analysis_prompt = ChatPromptTemplate.from_template("""
ì‚¬ìš©ìê°€ ìƒì„¸ ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.

ê¸°ì¡´ ëŒ€í™”: {chat_history}
ì‚¬ìš©ì ìš”ì²­: {user_input}

ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ” ë²•ë¥ ì  ë¶„ì„
- ì ìš© ê°€ëŠ¥í•œ ë„ë¡œêµí†µë²• ì¡°í•­
- ê³¼ì‹¤ ë¹„ìœ¨ ê²°ì • ìš”ì¸

## ğŸ“Š ê³¼ì‹¤ë¹„ìœ¨ ìƒì„¸ ë¶„ì„
- Aì°¨ëŸ‰ ê³¼ì‹¤ ìš”ì¸
- Bì°¨ëŸ‰ ê³¼ì‹¤ ìš”ì¸
- ì¡°ì • ê°€ëŠ¥í•œ ìš”ì†Œë“¤

## ğŸ›¡ï¸ ëŒ€ì‘ ì „ëµ
- ë³´í—˜ì‚¬ ëŒ€ì‘ ë°©ë²•
- í•„ìš”í•œ ì¦ê±° ìë£Œ
- ì£¼ì˜ì‚¬í•­

## â“ ì¶”ê°€ ê³ ë ¤ì‚¬í•­
ì •í™•í•œ íŒë‹¨ì„ ìœ„í•´ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì•Œë ¤ì£¼ì„¸ìš”.

í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.
""")
        
        # ì¶”ê°€ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
        self.follow_up_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ì¡°ì‚¬ì›ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ íŒŒì•…í•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”.

í˜„ì¬ê¹Œì§€ì˜ ì •ë³´:
{user_input}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ğŸ” ì¶”ê°€ ì •ë³´ í•„ìš”
ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ëª‡ ê°€ì§€ ë” ì•Œë ¤ì£¼ì„¸ìš”:

**1. [ì²« ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì‚¬ê³  ë‹¹ì‹œ ì •í™•í•œ ì‹œê°„ê³¼ ë‚ ì”¨ëŠ” ì–´ë– í–ˆë‚˜ìš”?

**2. [ë‘ ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì‚¬ê³  ì§€ì ì˜ ì‹ í˜¸ë“± ìƒí™©ì€ ì–´ë– í–ˆë‚˜ìš”?

**3. [ì„¸ ë²ˆì§¸ ì§ˆë¬¸]**
- ì˜ˆ: ì–‘ ì°¨ëŸ‰ì˜ ì£¼í–‰ ì†ë„ëŠ” ëŒ€ëµ ì–¼ë§ˆì˜€ë‚˜ìš”?

ì´ ì •ë³´ë“¤ì„ ì•Œë ¤ì£¼ì‹œë©´ í›¨ì”¬ ë” ì •í™•í•œ ê³¼ì‹¤ë¹„ìœ¨ ë¶„ì„ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
    
    def quick_diagnosis(self, user_input: str, image_analysis: str = "") -> Dict[str, Any]:
        """íŒë¡€ë¥¼ í™œìš©í•œ ë¹ ë¥¸ ì§„ë‹¨"""
        try:
            # ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
            similar_cases = self._search_similar_cases(user_input)
            
            # íŒë¡€ ì •ë³´ í¬ë§·íŒ…
            case_examples = ""
            reference_cases = ""
            similar_cases_text = "ê´€ë ¨ íŒë¡€ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            if similar_cases:
                case_examples = "\n".join([
                    f"- {case['case_id']}: {case['fault_ratio']}"
                    for case in similar_cases
                ])
                
                reference_cases = "\n".join([
                    f"**{case['case_id']}**: {case['accident_description'][:100]}... (ê³¼ì‹¤ë¹„ìœ¨: {case['fault_ratio']})"
                    for case in similar_cases
                ])
                
                similar_cases_text = "\n".join([
                    f"{case['case_id']}: {case['accident_description']} (ê³¼ì‹¤ë¹„ìœ¨: {case['fault_ratio']})"
                    for case in similar_cases
                ])
            
            chain = LLMChain(llm=self.llm, prompt=self.enhanced_diagnosis_prompt)
            
            result = chain.invoke({
                "user_input": user_input,
                "image_analysis": image_analysis or "ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì—†ìŒ",
                "similar_cases": similar_cases_text,
                "case_examples": case_examples or "ê´€ë ¨ íŒë¡€ ì—†ìŒ",
                "reference_cases": reference_cases or "ê´€ë ¨ íŒë¡€ ì—†ìŒ"
            })
            
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            self.memory.chat_memory.add_user_message(user_input)
            self.memory.chat_memory.add_ai_message(result["text"])
            
            return {
                "answer": result["text"],
                "type": "quick_diagnosis",  # UI í˜¸í™˜ì„±ì„ ìœ„í•´ quick_diagnosisë¡œ ë³€ê²½
                "status": "success",
                "similar_cases": similar_cases
            }
            
        except Exception as e:
            logger.error(f"íŒë¡€ í™œìš© ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "type": "error",
                "status": "error",
                "error": str(e)
            }
    
    def detailed_analysis(self, user_input: str) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„"""
        try:
            chat_history = "\n".join([
                f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ìë¬¸ì‚¬'}: {msg.content}"
                for msg in self.memory.chat_memory.messages[-6:]
            ])
            
            chain = LLMChain(llm=self.llm, prompt=self.detailed_analysis_prompt)
            
            result = chain.invoke({
                "chat_history": chat_history,
                "user_input": user_input
            })
            
            self.memory.chat_memory.add_user_message(user_input)
            self.memory.chat_memory.add_ai_message(result["text"])
            
            return {
                "answer": result["text"],
                "type": "detailed_analysis",
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "answer": "ìƒì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "type": "error",
                "status": "error",
                "error": str(e)
            }
    
    def generate_follow_up_questions(self, user_input: str, chat_history: str = "") -> Dict[str, Any]:
        """ì¶”ê°€ ì§ˆë¬¸ ìƒì„±"""
        try:
            chain = LLMChain(llm=self.llm, prompt=self.follow_up_prompt)
            
            result = chain.invoke({
                "user_input": user_input,
                "chat_history": chat_history or "ëŒ€í™” ê¸°ë¡ ì—†ìŒ"
            })
            
            return {
                "questions": result["text"],
                "type": "follow_up_questions",
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "questions": "ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "type": "error",
                "status": "error"
            }
    
    def analyze_with_context(self, user_input: str, image_analysis: str = "") -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„"""
        detail_keywords = ["ìƒì„¸", "ë” ìì„¸íˆ", "ìƒì„¸ ë¶„ì„", "ë” ì•Œê³  ì‹¶", "ìì„¸í•œ ì„¤ëª…"]
        is_detail_request = any(keyword in user_input for keyword in detail_keywords)
        
        if is_detail_request:
            return self.detailed_analysis(user_input)
        else:
            return self.quick_diagnosis(user_input, image_analysis)
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear()
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

def test_enhanced_advisor():
    """íŒë¡€ í™œìš© ìë¬¸ì‚¬ í…ŒìŠ¤íŠ¸"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    if not google_api_key:
        print("GOOGLE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        return
    
    advisor = EnhancedTrafficAdvisor(google_api_key)
    
    print("ğŸš— íŒë¡€ í™œìš© êµí†µì‚¬ê³  ìë¬¸ì‚¬ í…ŒìŠ¤íŠ¸")
    test_case = "ì‹ í˜¸ë“± êµì°¨ë¡œì—ì„œ ì§ì§„ ì¤‘ ì¢ŒíšŒì „ ì°¨ëŸ‰ê³¼ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤"
    
    result = advisor.quick_diagnosis(test_case)
    print("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!" if result["status"] == "success" else "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    if result["status"] == "success":
        print(f"ğŸ“š ê´€ë ¨ íŒë¡€: {len(result.get('similar_cases', []))}ê±´")

if __name__ == "__main__":
    test_enhanced_advisor()
