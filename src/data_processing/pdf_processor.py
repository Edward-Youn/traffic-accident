import os
import json
import re
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger

import PyPDF2
import fitz  # PyMuPDF
import pdfplumber
from docx import Document

class LegalDocumentProcessor:
    def __init__(self, input_dir="./data/laws", output_dir="./data/processed"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.input_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logger.add("./logs/pdf_processor.log", rotation="10 MB")
        
        # ë²•ë¥  ë¬¸ì„œ íŒ¨í„´ë“¤
        self.legal_patterns = {
            'article': r'ì œ(\d+)ì¡°\s*(?:\(([^)]+)\))?',  # ì œ1ì¡°(ëª©ì )
            'paragraph': r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # í•­ ë²ˆí˜¸
            'item': r'(\d+)\.',  # í˜¸ ë²ˆí˜¸
            'subitem': r'([ê°€-í£])\.',  # ëª© ë²ˆí˜¸
        }
    
    def process_all_documents(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë²•ë¥  ë¬¸ì„œ ì²˜ë¦¬"""
        all_documents = []
        
        logger.info(f"ë²•ë¥  ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {self.input_dir}")
        
        # ì§€ì› íŒŒì¼ í˜•ì‹
        supported_extensions = {'.pdf', '.docx', '.txt'}
        
        for file_path in self.input_dir.rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                try:
                    logger.info(f"ì²˜ë¦¬ ì¤‘: {file_path.name}")
                    document = self._process_single_document(file_path)
                    if document:
                        all_documents.append(document)
                except Exception as e:
                    logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        # ì²˜ë¦¬ëœ ë¬¸ì„œ ì €ì¥
        output_file = self.output_dir / "processed_legal_documents.json"
        self._save_processed_documents(all_documents, output_file)
        
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(all_documents)}ê°œ ë¬¸ì„œ")
        return all_documents
    
    def _process_single_document(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"""
        # íŒŒì¼ í™•ì¥ìë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if file_path.suffix.lower() == '.pdf':
            text = self._extract_pdf_text(file_path)
        elif file_path.suffix.lower() == '.docx':
            text = self._extract_docx_text(file_path)
        elif file_path.suffix.lower() == '.txt':
            text = self._extract_txt_text(file_path)
        else:
            return None
        
        if not text.strip():
            logger.warning(f"ë¹ˆ ë¬¸ì„œ: {file_path.name}")
            return None
        
        # ë²•ë¥  ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        parsed_content = self._parse_legal_structure(text)
        
        return {
            "filename": file_path.name,
            "file_path": str(file_path),
            "document_type": self._classify_document_type(file_path.name, text),
            "raw_text": text,
            "parsed_content": parsed_content,
            "metadata": {
                "file_size": file_path.stat().st_size,
                "processed_at": logger._get_time().isoformat() if hasattr(logger, '_get_time') else None,
                "article_count": len(parsed_content.get('articles', [])),
                "total_length": len(text)
            }
        }
    
    def _extract_pdf_text(self, file_path: Path) -> str:
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
        text = ""
        
        # ë°©ë²• 1: PyMuPDF (ê°€ì¥ ê°•ë ¥í•¨)
        try:
            doc = fitz.open(file_path)
            for page_num in range(doc.page_count):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text:
                    text += page_text + "\n"
            doc.close()
            
            if text.strip():
                logger.info(f"PyMuPDFë¡œ {file_path.name} ì¶”ì¶œ ì„±ê³µ")
                return self._clean_extracted_text(text)
        except Exception as e:
            logger.warning(f"PyMuPDF ì¶”ì¶œ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        # ë°©ë²• 2: pdfplumber (í…Œì´ë¸” ì²˜ë¦¬ ì¢‹ìŒ)
        try:
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if text.strip():
                logger.info(f"pdfplumberë¡œ {file_path.name} ì¶”ì¶œ ì„±ê³µ")
                return self._clean_extracted_text(text)
        except Exception as e:
            logger.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        # ë°©ë²• 3: PyPDF2 (ë§ˆì§€ë§‰ ì‹œë„)
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if text.strip():
                logger.info(f"PyPDF2ë¡œ {file_path.name} ì¶”ì¶œ ì„±ê³µ")
                return self._clean_extracted_text(text)
        except Exception as e:
            logger.error(f"PyPDF2 ì¶”ì¶œ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        logger.error(f"ëª¨ë“  PDF ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨: {file_path.name}")
        return ""
    
    def _extract_docx_text(self, file_path: Path) -> str:
        """DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        try:
            doc = Document(file_path)
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
        except Exception as e:
            logger.error(f"DOCX ì¶”ì¶œ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        return self._clean_extracted_text(text)
    
    def _extract_txt_text(self, file_path: Path) -> str:
        """TXT íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
        except UnicodeDecodeError:
            # UTF-8ì´ ì•ˆ ë˜ë©´ CP949 ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as file:
                    text = file.read()
            except Exception as e:
                logger.error(f"TXT ì½ê¸° ì‹¤íŒ¨ {file_path.name}: {e}")
                return ""
        except Exception as e:
            logger.error(f"TXT ì½ê¸° ì‹¤íŒ¨ {file_path.name}: {e}")
            return ""
        
        return self._clean_extracted_text(text)
    
    def _clean_extracted_text(self, text: str) -> str:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n\s*\n', '\n\n', text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì œê±°
        text = re.sub(r' +', ' ', text)  # ì—°ì†ëœ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def _parse_legal_structure(self, text: str) -> Dict[str, Any]:
        """ë²•ë¥  ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±"""
        parsed = {
            'articles': [],
            'structure_info': {
                'has_articles': False,
                'has_paragraphs': False,
                'has_items': False
            }
        }
        
        # ì¡°ë¬¸ ì¶”ì¶œ
        articles = self._extract_articles(text)
        parsed['articles'] = articles
        
        if articles:
            parsed['structure_info']['has_articles'] = True
        
        # í•­, í˜¸ í™•ì¸
        if re.search(self.legal_patterns['paragraph'], text):
            parsed['structure_info']['has_paragraphs'] = True
        
        if re.search(self.legal_patterns['item'], text):
            parsed['structure_info']['has_items'] = True
        
        return parsed
    
    def _extract_articles(self, text: str) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        
        # ì¡°ë¬¸ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
        article_matches = list(re.finditer(self.legal_patterns['article'], text))
        
        for i, match in enumerate(article_matches):
            article_num = match.group(1)
            article_title = match.group(2) if match.group(2) else ""
            
            # ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ (ë‹¤ìŒ ì¡°ë¬¸ê¹Œì§€)
            start_pos = match.start()
            if i + 1 < len(article_matches):
                end_pos = article_matches[i + 1].start()
                content = text[start_pos:end_pos].strip()
            else:
                content = text[start_pos:].strip()
            
            articles.append({
                'article_number': int(article_num),
                'title': article_title,
                'content': content,
                'paragraphs': self._extract_paragraphs(content)
            })
        
        return articles
    
    def _extract_paragraphs(self, article_content: str) -> List[str]:
        """ì¡°ë¬¸ ë‚´ í•­ ì¶”ì¶œ"""
        paragraphs = []
        
        # í•­ ë²ˆí˜¸ë¡œ ë¶„í• 
        paragraph_pattern = self.legal_patterns['paragraph']
        parts = re.split(f'({paragraph_pattern})', article_content)
        
        current_paragraph = ""
        for part in parts:
            if re.match(paragraph_pattern, part):
                if current_paragraph.strip():
                    paragraphs.append(current_paragraph.strip())
                current_paragraph = part
            else:
                current_paragraph += part
        
        if current_paragraph.strip():
            paragraphs.append(current_paragraph.strip())
        
        return paragraphs
    
    def _classify_document_type(self, filename: str, text: str) -> str:
        """ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜"""
        filename_lower = filename.lower()
        text_sample = text[:1000].lower()
        
        if 'ë„ë¡œêµí†µë²•' in text_sample or 'road traffic' in filename_lower:
            return "ë„ë¡œêµí†µë²•"
        elif 'ìë™ì°¨ì†í•´ë°°ìƒ' in text_sample:
            return "ìë™ì°¨ì†í•´ë°°ìƒë³´ì¥ë²•"
        elif 'íŒë¡€' in filename_lower or 'case' in filename_lower:
            return "íŒë¡€"
        elif 'ì‹œí–‰ë ¹' in text_sample:
            return "ì‹œí–‰ë ¹"
        elif 'ì‹œí–‰ê·œì¹™' in text_sample:
            return "ì‹œí–‰ê·œì¹™"
        else:
            return "ê¸°íƒ€ë²•ë ¹"
    
    def _save_processed_documents(self, documents: List[Dict[str, Any]], output_file: Path):
        """ì²˜ë¦¬ëœ ë¬¸ì„œ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(documents, f, ensure_ascii=False, indent=2)
            logger.info(f"ì²˜ë¦¬ëœ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {output_file}")
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    processor = LegalDocumentProcessor()
    
    print("ğŸ“„ ë²•ë¥  ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: {processor.input_dir}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {processor.output_dir}")
    
    # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    if not any(processor.input_dir.iterdir()):
        print("âš ï¸ ì…ë ¥ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ë‹¤ìŒ ê²½ë¡œì— PDF, DOCX, TXT íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”: {processor.input_dir}")
        return
    
    # íŒŒì¼ ëª©ë¡ ì¶œë ¥
    files = list(processor.input_dir.rglob('*'))
    supported_files = [f for f in files if f.suffix.lower() in {'.pdf', '.docx', '.txt'}]
    
    print(f"ğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ({len(supported_files)}ê°œ):")
    for file in supported_files:
        print(f"  - {file.name}")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    documents = processor.process_all_documents()
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(documents)}ê°œ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()