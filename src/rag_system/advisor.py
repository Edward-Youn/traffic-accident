"""
êµí†µì‚¬ê³  ìë¬¸ ì±—ë´‡ ì‹œìŠ¤í…œ
LangChainì„ í™œìš©í•œ RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
"""
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage
from loguru import logger

from .document_processor import DocumentProcessor

class TrafficAccidentAdvisor:
    def __init__(self, google_api_key: str, model_name: str = "gemini-1.5-flash"):
        """
        êµí†µì‚¬ê³  ìë¬¸ ì±—ë´‡ ì´ˆê¸°í™”
        
        Args:
            google_api_key: Google API í‚¤
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.google_api_key = google_api_key
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=google_api_key,
            temperature=0.3,
            convert_system_message_to_human=True
        )
        
        # ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.doc_processor = DocumentProcessor(google_api_key)
        self.vectorstore = self.doc_processor.load_vectorstore()
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ConversationBufferWindowMemory(
            k=5,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self._setup_prompts()
        
        # ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”
        self._setup_chains()
        
        logger.info("êµí†µì‚¬ê³  ìë¬¸ ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        
        # ë©”ì¸ ìë¬¸ í”„ë¡¬í”„íŠ¸
        self.advisor_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ë²•ë¥  ìë¬¸ì‚¬ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ êµí†µì‚¬ê³  ìƒí™©ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë¥ ê³¼ íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.

ê´€ë ¨ ìë£Œ:
{context}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **ìƒí™© ë¶„ì„**: ì‚¬ìš©ìê°€ ì œê³µí•œ ì‚¬ê³  ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…
2. **ê´€ë ¨ ë²•ë¥ **: ì ìš© ê°€ëŠ¥í•œ ë„ë¡œêµí†µë²• ë° ê´€ë ¨ ê·œì • ì„¤ëª…
3. **ìœ ì‚¬ íŒë¡€**: ì œê³µëœ ìë£Œì—ì„œ ìœ ì‚¬í•œ ì‚¬ê³  ì‚¬ë¡€ì™€ ê³¼ì‹¤ ë¹„ìœ¨ ì°¸ì¡°
4. **ì˜ˆìƒ ê³¼ì‹¤ë¹„ìœ¨**: ìƒí™©ì— ë”°ë¥¸ ì˜ˆìƒ ê³¼ì‹¤ ë¹„ìœ¨ ì œì‹œ
5. **ì¡°ì¹˜ ì‚¬í•­**: ì·¨í•´ì•¼ í•  êµ¬ì²´ì ì¸ í–‰ë™ ë°©ì•ˆ ì œì‹œ
6. **ì£¼ì˜ì‚¬í•­**: ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤

**ì¤‘ìš”**: 
- ì •í™•í•œ ë²•ë¥  ì¡°ì–¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´ì´ í•„ìš”í•¨ì„ í•­ìƒ ëª…ì‹œ
- ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŒì„ ì¸ì •
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ

ë‹µë³€:
""")
        
        # ìƒí™© íŒŒì•…ìš© í”„ë¡¬í”„íŠ¸
        self.clarification_prompt = ChatPromptTemplate.from_template("""
ì‚¬ìš©ìê°€ êµí†µì‚¬ê³  ìƒí™©ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. 
ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥: {user_input}

í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™”:
{chat_history}

ë‹¤ìŒ ì •ë³´ë“¤ì´ ì¶©ë¶„íˆ ì œê³µë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:

í•„ìˆ˜ ì •ë³´:
- ì‚¬ê³  ë°œìƒ ì¥ì†Œ ë° ë„ë¡œ ìƒí™©
- ì‚¬ê³  ë‹¹ì‹œ ì–‘ ì°¨ëŸ‰ì˜ í–‰ë™ (ì§ì§„, ì¢ŒíšŒì „, ìš°íšŒì „, ì •ì°¨ ë“±)
- êµí†µì‹ í˜¸ ìƒí™©
- ì‚¬ê³  ê²½ìœ„ ë° ì¶©ëŒ ë¶€ìœ„
- ë¶€ìƒ ì—¬ë¶€ ë° ì •ë„
- ë³´í—˜ ì²˜ë¦¬ ìƒí™©

ë§Œì•½ ì¶©ë¶„í•œ ì •ë³´ê°€ ì œê³µë˜ì—ˆë‹¤ë©´ "ì •ë³´ ì¶©ë¶„"ì´ë¼ê³  ë‹µí•˜ê³ ,
ë¶€ì¡±í•˜ë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ê°€ ë” í•„ìš”í•œì§€ ì¹œê·¼í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.
""")
    
    def _setup_chains(self):
        """ëŒ€í™” ì²´ì¸ ì„¤ì •"""
        if self.vectorstore:
            # RAG ì²´ì¸ ì„¤ì •
            self.qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vectorstore.as_retriever(search_kwargs={"k": 5}),
                memory=self.memory,
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": self.advisor_prompt}
            )
        else:
            logger.warning("ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì–´ì„œ ì¼ë°˜ LLM ì²´ì¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
            self.qa_chain = None
        
        # ìƒí™© íŒŒì•… ì²´ì¸
        self.clarification_chain = LLMChain(
            llm=self.llm,
            prompt=self.clarification_prompt
        )
    
    def analyze_situation(self, user_input: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ ìƒí™© ë¶„ì„"""
        try:
            chat_history = self.memory.chat_memory.messages
            chat_history_str = "\n".join([
                f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ìë¬¸ì‚¬'}: {msg.content}"
                for msg in chat_history[-10:]  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ
            ])
            
            result = self.clarification_chain.run(
                user_input=user_input,
                chat_history=chat_history_str
            )
            
            return {
                "needs_clarification": "ì •ë³´ ì¶©ë¶„" not in result,
                "clarification_questions": result,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"ìƒí™© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "needs_clarification": False,
                "clarification_questions": "",
                "status": "error",
                "error": str(e)
            }
    
    def get_advice(self, question: str) -> Dict[str, Any]:
        """êµí†µì‚¬ê³  ìë¬¸ ì œê³µ"""
        try:
            if self.qa_chain:
                # RAG ê¸°ë°˜ ë‹µë³€
                result = self.qa_chain({
                    "question": question
                })
                
                return {
                    "answer": result["answer"],
                    "source_documents": [
                        {
                            "content": doc.page_content[:300] + "...",
                            "metadata": doc.metadata
                        }
                        for doc in result.get("source_documents", [])
                    ],
                    "status": "success"
                }
            else:
                # ì¼ë°˜ LLM ë‹µë³€ (ë²¡í„°DB ì—†ì„ ë•Œ)
                general_prompt = f"""
ë‹¹ì‹ ì€ êµí†µì‚¬ê³  ì „ë¬¸ ìë¬¸ì‚¬ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ êµí†µë²•ê·œì™€ ìƒì‹ì„ ì—ì„œ ì¡°ì–¸í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

**ì£¼ì˜**: ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤.
"""
                response = self.llm.invoke([HumanMessage(content=general_prompt)])
                
                return {
                    "answer": response.content,
                    "source_documents": [],
                    "status": "success"
                }
                
        except Exception as e:
            logger.error(f"ìë¬¸ ì œê³µ ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "source_documents": [],
                "status": "error",
                "error": str(e)
            }
    
    def get_conversation_summary(self) -> str:
        """ëŒ€í™” ìš”ì•½ ì œê³µ"""
        try:
            messages = self.memory.chat_memory.messages
            if not messages:
                return "ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
            
            conversation_text = "\n".join([
                f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ìë¬¸ì‚¬'}: {msg.content}"
                for msg in messages
            ])
            
            summary_prompt = f"""
ë‹¤ìŒì€ êµí†µì‚¬ê³  ë²•ë¥  ìë¬¸ ëŒ€í™”ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

{conversation_text}

ìš”ì•½ í˜•ì‹:
- ì‚¬ê³  ìƒí™©: 
- ì£¼ìš” ìŸì :
- ì œê³µëœ ì¡°ì–¸:
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­:
"""
            
            response = self.llm.invoke([HumanMessage(content=summary_prompt)])
            return response.content
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "ëŒ€í™” ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear()
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    if not google_api_key:
        print("GOOGLE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        return
    
    # ìë¬¸ì‚¬ ì´ˆê¸°í™”
    advisor = TrafficAccidentAdvisor(google_api_key)
    
    print("ğŸš— êµí†µì‚¬ê³  ìë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.")
    print("ì‚¬ê³  ìƒí™©ì„ ì„¤ëª…í•´ì£¼ì‹œë©´ ë²•ë¥  ìë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
    print("'quit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")
    
    while True:
        user_input = input("\nì‚¬ìš©ì: ")
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            break
        
        # ìƒí™© ë¶„ì„
        analysis = advisor.analyze_situation(user_input)
        
        if analysis["needs_clarification"]:
            print(f"\nìë¬¸ì‚¬: {analysis['clarification_questions']}")
        else:
            # ìë¬¸ ì œê³µ
            advice = advisor.get_advice(user_input)
            print(f"\nìë¬¸ì‚¬: {advice['answer']}")
            
            if advice['source_documents']:
                print("\nğŸ“š ì°¸ê³  ìë£Œ:")
                for i, doc in enumerate(advice['source_documents'][:2], 1):
                    print(f"{i}. {doc['metadata'].get('case_id', 'N/A')}: {doc['content'][:100]}...")

if __name__ == "__main__":
    main()
